---
title: CAVEDB-1: Creating a key-value database in C++
author: Silesh Chandran
date: 2021-09-19 18:10:00 +0530
categories: [backend, projects]
tags: [backend, c++, database, dbms, c#, projects, nosql]
toc: true
comments: true
---

Welcome to this new series of posts in which I will delve into creating a database from scratch in C++. Lets begin by setting expectations and defining the scope of this project. In the next secion I will elaborate on how the data is actually stored. I plan to add more complexity in upcoming posts and also build an API layer on top of this to do CRUD operations over the network.

## Features expected:
1. This will support storing a simple string not limited by size (so a single key-value field)
2. The 'DB' will consist of a single DB file and an index file.
3. Index will be initally an in memory map, but needs to be optimized to a disk based tree
4. Operations supported: Add, retrieve, get all documents. Delete and Update will be considered later.

## Development plan:

In this post I'll build a protype that can act as an object store. So this simply stores what value is given and does not care about the data. This means we will not be able to perform queries on it, but that can come later.

## Data Storage in DB files

The most intresting part of building a DB in my opinion is figuring out how we can persist the data in disk and do it in a way that allows for easy storage and retrival. The naive approach to this is to use multiple files. Each document can have its own file. This simplifies retrival because we can simply rely on the OS to fetch us the document given the document name (which can be a key).

This is pretty efficient, but its also too simple. The better way to do it would be to have a single db file and a separate index file which will tell us the location of a document within that file. In my understanding this is similar to how most DBMS handle storage.

This means we would need to come up with a specification on how each document needs to be stored. We also need to know when a document starts and ends, and how large it is so that it can be read efficiently. Lets also cap the max size of the document to 4MB, which translates to 2^12 bytes, so 4096 characters in total. We will store the size of the document as well to facilitate retrival.

#### Structure of DB File
So each document will contain a start character, 2 bytes for a short int storing the size of the document, the list of key-values separated by a character and an end character.
Start and end character: ```~```
Size byte: ```A-M (2^3 to 2^16)```
Document data: ```"key1":"value","key2":"value"```

Example:
```
{
	"empId": "EVA_01",
	"empName": "Ikari Shinji"
}
```
We will store the values size of the document, then the data itself. It will be enclosed in '~' to serve as a separator. So in the db file, the data will look like:
```
......~ 29 "key1":"value","key2":"value" ~......
````

### Structure of Index file:

The index file contains information about where in the db file the document is stored. We can also add other information that we might need, for now I'm only storing the Db name and the index details.
To find a document in the DB file, we need to know its location. So in the index we store the key and the location. This is loaded into memory on startup and persisted back to disk intermittently. This is not the most efficient way to implement an index. When we have too many documents we cannot keep the index in memory all the time. Hence we will switch to a B-tree structure that will net us log(n) read times for a particular document.

Since we index based on a key, this is mandatory for all documents. It is also necessary that the key be unique. These constraints need to be enforced when adding documents. We also store a single digit (0 or 1) depending on if the document is available or deleted.

Example:
If we have 3 documents in the DB, with Ids: EVA_01, EVA_00 and EVA_02, then our index will be:
```
EVA_00 0 1
EVA_01 64 1
EVA_02 128 1
```

So at startup this index is loaded into memory, and every time a put request happens, we update both the in memory index as well as the one on disk. This is extremely simple and quite performant as well, since lookup and addition are both happening in O(1). Only non deleted values (1) will be loaded into memory.
The major downside to this approach is that the index will have to stay in memory for as long as we need the db to be in use. This affects how fast it can startup and in cases where the index grows too large consume too much memory. If we want a DB that does not rely on an index, then we will have to find a compromise. One solution is to use an AVL tree in disk to be the index. This way, the index can be read directly from the file and get and put requests are both done in O(logn) time. I will explore this idea in the next post.

#### Deletion issue
Having documents of varying Sizes also comes with a new challenge when deletion enters the picture. Since we only have one file, deleting a document would mean everything stored after it will need to be shifted. This is extremely inefficient and is a performance hit. We can instead mark a document as deleted and keep track of the free space in a separate file. So this can be reused when a document with similar size is to be added. This can increase the likelyhood of internal fragmentation and we can lessen it by always allocating 2^n bytes for a document. So if a document needs 900 bytes, we will allocate 1024.

## Working and project structure
The project has 2 source files, and 2 corresponding headers. The caveDB.h and caveDB.cpp files contain the definition of the DB class and exposes the put and get methods. The storage.cpp contains definitions for methods that are used to read data and store data. This separation makes it so that we can change the way file handling works without touching the db code.
On startup, the user is asked to enter the name of the db. This loads an existing db if the files are already available, otherwise it creates one. The DB and index files are created. A singleton instance of the DB is created and the index file is loaded into memory as a map. This instance can now be used to get and put data into the db. Upon insertion, both the in memory as well as the on disk index is updated.

This is a very simplistic approach but it is a good starting point. I have also tried to write some tests using Catch2. The driver main function allows for accessing the DB via a command line interface. So that needs to be improved on so that this can be interfaced in other ways as well.

In the main method, we read the db name and the path where the db is to be created/read from. If a db already exists then its read and the index is loaded in memory, otherwise a folder with the db name is created and empty files created for the db and the index. From here we can insert, fetch and fetch all data:

![Desktop View](/assets/img/CAVEDB/test2_tr_ccezT2hM.png)
_Creating a new DB and inserting 2 elements_


![Desktop View](/assets/img/CAVEDB/test1_sawAnOWmf3.png)
_Loading the existing DB and reading values_

#### contents of DB file:
```
12 data for k1~16 "Data for KEY2"~
```

#### contents of index file:
```
k1 0
k2 15
```

## Whats next?

Two things come to mind immediately that needs to be addressed. For this DB to be usable we need a way to avoid loading the entire index into memory. The way to do this would be to store the index in a searchable format. I am considering AVL trees or B trees for this.
The second one is that the DB does not understand the data, hence no querying can be done. This can be remedied by defining a structure that the data needs to follow (like json or key-value). The metadata about the keys should be made availabe in the index and then this information can be used for querying. I have no idea how complex this would be but this is something that needs to be done.

The source code can be found here: https://github.com/caveman96/CAVEDB

Thanks for reading!


